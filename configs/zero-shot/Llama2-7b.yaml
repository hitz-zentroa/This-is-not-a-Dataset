#Model args
model_name_or_path: meta-llama/Llama-2-7b-chat-hf
torch_dtype: "bfloat16"
quantization: 4
force_auto_device_map: false
predict_with_generate: false
per_device_eval_batch_size: 32
use_flash_attention: true
fewshot: false

# dataset arguments
do_train: false
do_eval: false
do_predict: true
do_predict_full_dataset: true
max_seq_length: 4096


# Output Dir
output_dir: results/zero-shot/llama-2-7b-chat-hf


