#Model args
model_name_or_path: google/flan-t5-xxl
torch_dtype: "auto"
use_lora: true
quantization: 4
force_auto_device_map: false
predict_with_generate: false
conversation_template: null

# Dataset arguments
do_train: true
do_eval: true
do_predict: true
do_predict_full_dataset: false
max_seq_length: 512

pattern: null
only_affirmative: False
only_negated: False
only_non_distractor: False
only_distractor: False

#Training arguments
per_device_train_batch_size: 32
gradient_accumulation_steps: 1
per_device_eval_batch_size: 32
optim: paged_adamw_32bit
learning_rate: 0.0003
weight_decay: 0
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.03

# Output Dir
output_dir: results/finetune/flan-t5-xxl


